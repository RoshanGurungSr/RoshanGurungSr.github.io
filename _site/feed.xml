<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-06-27T05:42:11+05:45</updated><id>http://localhost:4000/feed.xml</id><title type="html">Roshan Gurung</title><subtitle>A personal blog to share my learnings and experiences</subtitle><author><name>Roshan Gurung</name></author><entry><title type="html">Understanding working mechanism of Decision Tree (ID3 Variant)</title><link href="http://localhost:4000/machine%20learning/decision_tree/" rel="alternate" type="text/html" title="Understanding working mechanism of Decision Tree (ID3 Variant)" /><published>2024-06-04T17:45:30+05:45</published><updated>2024-06-04T17:45:30+05:45</updated><id>http://localhost:4000/machine%20learning/decision_tree</id><content type="html" xml:base="http://localhost:4000/machine%20learning/decision_tree/"><![CDATA[<p>Coming Soon…</p>]]></content><author><name>Roshan Gurung</name></author><category term="Machine Learning" /><summary type="html"><![CDATA[Being Worked On. Please Be Patience and Wait.]]></summary></entry><entry><title type="html">Linear Regression with OLS from scratch</title><link href="http://localhost:4000/machine%20learning/linear_regression/" rel="alternate" type="text/html" title="Linear Regression with OLS from scratch" /><published>2024-06-04T17:45:30+05:45</published><updated>2024-06-04T17:45:30+05:45</updated><id>http://localhost:4000/machine%20learning/linear_regression</id><content type="html" xml:base="http://localhost:4000/machine%20learning/linear_regression/"><![CDATA[<p>Linear Regression is a statistical model and a supervised learning algorithm used for predicting a continuous target variable based on one or more predictor variables. The target variable is the final output that we are trying to estimate and the predictor variables are the features of the data. The objective is to find a best-fitting line that minimizes the difference between predicted values, and actual values.</p>

<p>Simple Linear Regression consists of a single predictor variable “x” and a response variable “y”. It is modeled by the linear equation:</p>

\[y = \beta_0 + \beta_1x + \epsilon \tag{1} \label{eq:simple-lr}\]

<p>The objective of Simple Linear Regression using the Ordinary Least Square (OLS) method is to find the values of β0 and β1 that minimize the sum of squared differences between the observed value and values predicted. The Sum of Square Error (SSE) is given by:</p>

\[SSE = \sum_{i=1}^n (y_i - \hat{y_i})^2 \tag{2} \label{eq:sse}\]

<h4 id="1-derivation-of-beta_0-and-beta_1">1. Derivation of $\beta_0$ and $\beta_1$</h4>
<p>Since our objective is to minimize the SSE, partial derivative w.r.t $\beta_0$ and $\beta_1$ is taken, set them to 0, then we will solve for coeddicients. <br />
Using $\eqref{eq:simple-lr}$ and $\eqref{eq:sse}$, we get:</p>

\[SSE = \sum_{i=1}^n (y_i - \beta_0 -\beta_1*x_0)^2\]

<p>Taking partial derivative w.r.t $\beta_0$</p>

\[\frac{\partial SSE}{ \partial \beta_0} = \frac{\partial \sum_{i=1}^n (y_i - \beta_0 - \beta_1*x_i)^2}{\partial \beta_0}\]

<p>Using chain rule:</p>

\[\frac{\partial SSE}{ \partial \beta_0} = 2 * \sum_{i=1}^n (y_i - \beta_0 - \beta_1x_i) * \frac{\partial \sum_{i=1}^n (y_i - \beta_0 - \beta_1x_i)}{\partial \beta_0}\]

\[= 2 * \sum_{i=1}^n (y_i - \beta_0 - \beta_1x_i)\]

\[= -2 * \sum_{i=1}^n(y_i - \beta_0 - \beta_1x_i)\]

<p>Setting partial derivative to 0:</p>

\[\Rightarrow -2 * \sum_{i=1}^n(y_i - \beta_0 - \beta_1x_i) = 0\]

\[\Rightarrow \sum_{i=1}^n(y_i - \beta_0 - \beta_1x_i) = 0\]

\[\Rightarrow \sum_{i=1}^ny_i - n\beta_0 - \beta_1\sum_{i=1}^nx_i = 0\]

\[\Rightarrow n\beta_0 = \sum_{i=1}^n - \beta_1\sum{i=1}^nx_i\]

\[\Rightarrow \beta_0 = \frac{\sum_{i=1}^n y_i - \beta_1\sum_{i=1}^nx_i}{n}\]

\[\Rightarrow \beta_0 = \bar{y} - \beta_1\bar{x} \tag{3} \label{eq:beta0}\]

<p>Taking partial derivative w.r.t $\beta_1$</p>

\[\frac{\partial SSE}{\partial \beta_1} = \frac{\partial \sum_{i=1}^n(y_i - \beta_0 - \beta_1x_i)^2}{\partial \beta_1}\]

<p>Using chain rule:</p>

\[\frac{\partial SSE}{\partial \beta_1} = -2*\sum_{i=1}^nx_i * (y_i - \beta_0 - \beta_1x_i)\]

<p>Setting partial derivative to 0:</p>

\[\Rightarrow -2 * \sum_{i=1}^nx_i * (y_i - \beta_0 - \beta_1x_i) = 0\]

\[\Rightarrow \sum_{i=1}^nx_i * (y_i - \beta_0 - \beta_1x_i) = 0\]

\[\Rightarrow \sum_{i=1}^nx_iy_i - \beta_0\sum_{i=1}^nx_i - \beta_1\sum_{i=1}^nx_i^2 = 0\]

<p>Substituting $\beta_0$,</p>

\[\Rightarrow \sum_{i=1}^nx_iy_i - (\bar{y} - \beta_1\bar{x})\sum_{i=1}^nx_i - \beta_1\sum_{i=1}^nx_i^2 = 0\]

\[\Rightarrow \sum_{i=1}^nx_iy_i - \bar{y}\sum_{i=1}^nx_i + \beta_1\bar{x}\sum_{i=1}^nx_i - \beta_1\sum_{i=1}^nx_i^2 = 0\]

<p>Dividing both sides by n,</p>

\[\Rightarrow n\sum_{i=1}^nx_iy_i - n\bar{y}\sum_{i=1}^nx_i + n\beta_1\bar{x}\sum_{i=1}^nx_i - n\beta_1\sum_{i=1}^nx_i^2 = 0\]

<p>We know for the properties of mean,</p>

\[\sum_{i=1}^nx_i = n\bar{x}\]

\[\sum_{i=1}^ny_i = n\bar{y}\]

<p>Now, using above property,</p>

\[\Rightarrow \sum_{i=1}^nx_iy_i - \sum_{i=1}^nx_i\bar{y} = \beta_1(\sum_{i=1}^nx_i^2 - \sum_{i=1}^nx_i\bar{x})\]

\[\Rightarrow \sum_{i=1}^n(x_iy_i - \bar{y}x_i) = \beta_1\sum_{i=1}^n(x_i^2 - \bar{x}x_i) \tag{4} \label{eq:beta1-dev4}\]

<p>We know, for covariance of x and y:</p>

\[Cov(x, y) = \frac{1}{n}*\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})\]

\[=\frac{1}{n}\sum_{i=1}^n(x_iy_i - x_i\bar{y} - \bar{x}y_i + \bar{x}\bar{y})\]

\[=\frac{1}{n}(\sum_{i=1}^nx_iy_i - \bar{y}\sum_{i=1}^nx_i - \bar{x}\sum_{i=1}^ny_i + \bar{x}\bar{y}\sum_{i=1}^n1)\]

\[=\frac{1}{n}(\sum_{i=1}^nx_iy_i - \bar{y}*n\bar{x} - \bar{x}*n\bar{y} + n\bar{x}\bar{y})\]

\[=\frac{1}{n}(\sum_{i=1}^nx_iy_i - n\bar{x}\bar{y} - n\bar{x}\bar{y} + n\bar{x}\bar{y})\]

\[\Rightarrow \frac{1}{n}(\sum_{i=1}^nx_iy_i - n\bar{x}\bar{y}) = \frac{1}{n}(\sum_{i=1}^nx_iy_i - \bar{x}\bar{y}\sum_{i=1}^n1)\]

\[\therefore nCov(x,y) = \sum_{i=1}^n(x_iy_i - \bar{x}\bar{y})\]

<p>Again,</p>

\[Var(x) = \frac{1}{n}\sum_{i=1}^n(x_i - \bar{x})^2\]

\[= \frac{1}{n}\sum_{i=1}^n(x_i^2 - 2x_i\bar{x} + \bar{x}^2)\]

\[= \frac{1}{n}(\sum_{i=1}^nx_i^2 - 2\bar{x}\sum_{i=1}^nx_i + \bar{x}^2\sum_{i=1}^n1)\]

\[= \frac{1}{n}(\sum_{i=1}^nx_i^2 - 2\bar{x}\sum_{i=1}^nx_i + n\bar{x}^2)\]

\[= \frac{1}{n}(\sum_{i=1}^nx_i^2 - n\bar{x}^2)\]

\[= \frac{1}{n}(\sum_{i=1}^nx_i^2 - x^2\sum_{i=1}^n1)\]

\[\therefore nVar(x) = \sum_{i=1}^n(x_i^2 - \bar{x}^2)\]

<p>We know from equation $\eqref{eq:beta1-dev4}$,</p>

\[\Rightarrow \sum_{i=1}^n(x_iy_i - \bar{y}n\bar{x}) = \beta_1\sum_{i=1}^n(x_i^2 - \bar{x}*n\bar{x})\]

\[\Rightarrow n\sum_{i=1}^n(x_iy_i - \bar{y}\bar{x}) = n\beta_1\sum_{i=1}^n(x_i^2 - \bar{x}^2)\]

\[\Rightarrow nCov(x, y) = \beta_1*nVar(x)\]

\[\Rightarrow nCov(x, y) = \beta_1*nVar(x)\]

\[\therefore \beta_1 = \frac{Cov(x, y)}{Var(x)}\]

<p>Expanding Cov(x, y) and Var(x),</p>

\[\beta_1 = \frac{\frac{1}{n}\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{\frac{1}{n}\sum_{i=1}^n(x_i - \bar{x})^2}\]

\[\therefore \beta_1 = \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}\]

\[\therefore \beta_0 = \bar{y} - \beta_1\bar{x}\]

<p>By solving $\beta_0$ and $\beta_1$ in $\eqref{eq:simple-lr}$, we can calculate the $\hat{y}$. Using $\hat{y}$, we can calculate the SSE, which represents the accuarcy of our simple linear regression.</p>]]></content><author><name>Roshan Gurung</name></author><category term="Machine Learning" /><summary type="html"><![CDATA[Implementation of Linear Regression using OLS technique with mathematical derivation.]]></summary></entry><entry><title type="html">Kubeflow Setup On Local Machine</title><link href="http://localhost:4000/linux/mlops/test/" rel="alternate" type="text/html" title="Kubeflow Setup On Local Machine" /><published>2022-06-05T17:45:30+05:45</published><updated>2022-06-05T17:45:30+05:45</updated><id>http://localhost:4000/linux/mlops/test</id><content type="html" xml:base="http://localhost:4000/linux/mlops/test/"><![CDATA[<p>Coming Soon…</p>]]></content><author><name>Roshan Gurung</name></author><category term="linux" /><category term="MLOps" /><summary type="html"><![CDATA[Being Worked On. Please Be Patience and Wait.]]></summary></entry></feed>